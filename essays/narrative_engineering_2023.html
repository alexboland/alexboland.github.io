<!DOCTYPE html>
<html lang="en">
<head>
    <title>Narrative Engineering</title>
    <link href="https://fonts.googleapis.com/css?family=Lora|Montserrat|Raleway|Open+Sans|Crimson+Text&display=swap" rel="stylesheet">
    <link rel = "stylesheet"
   type = "text/css"
   href = "../style.css" />
</head>
<body>

<div class="essay">

<h1>Narrative Engineering (2023)</h1>

<p>From a certain standpoint, one could claim Theory to be a failed experiment. Although undeniably invaluable in some areas (namely physics) the attempt to apply our alleged abstract understanding of the world to predicting and controlling various problems has had a checkered track record. In some cases, it may be inherently self-contradictory: any economic theory that could give one an edge in the markets would be priced in once everybody is aware of it, and even those that are not necessarily meant to be instrumental in this way, such as macroeconomic forecasting, have not fared much better &mdash; possibly because these too are reflexive in the same way.&nbsp;&nbsp;</p>
<p>Others, like cognitive psychology, or psychoanalysis, which makes zero pretense of being empirical in the way we normally understand the word, may yet be helpful in some way, but draw completely understandable skepticism. Some will swear by psychotherapy while others may damn it, but from an outside point of view it is hard to tell what it is accomplishing or whether it is even harmful to the patient when in many cases it seems to go on indefinitely and when over a lifetime there are too many causal factors to find any reliable pattern that corresponds to the treatment in particular.</p>
<p>Biology, on the other hand, seems to do quite a bit better. One could bring up the importance of randomized control trials, but it is evident that even in a world without them there are discoveries and inventions that are powerful enough that they would be adopted and accepted as reliable as they become more &ldquo;obvious&rdquo;: penicillin would still have seen widespread adoption in the same way that a useful invention will simply get picked up by more people as more people benefit from it in a relatively direct way. Even psychotropic drugs, which are often much more equivocal in their ultimate efficacy, tend to have a way of dramatically altering one&rsquo;s experience and behaviors in the short term that gives them a distinct phenomenology to go by.</p>
<p>But to say that such a success in practice is due to <em>theory</em> is an unavoidably contentious statement. There are without a doubt plenty of <em>plausible </em>causal mechanisms that could explain disparate specifics from a much smaller set of more general principles, but one cannot simply <em>assume</em> that medicinal breakthroughs are fundamentally due to these explanations. One may, for example, hear about an excess or paucity of insulin or leptin determining the efficiency of one&rsquo;s metabolism and about certain foods being more likely to make someone fat or thin due to how much they provoke such hormones. Yet whether or not there are correlations or associations to justify these explanations, a quick look at the current state of knowledge about human metabolic pathways shows just how much this kind of talk ultimately resembles now scoffed-at just-so stories along the lines of the balance of the four humors:</p>

<img alt="" src="https://alexboland.s3.amazonaws.com/images/metabolism.jpg" style="width: 624.00px; height: 433.33px;">

<p>To be fair, one cannot straw-man serious biological research by criticizing unofficial folklore, but it does open up the question of how one can distill some kind of mechanism of action from countless potentially interacting parts. It is possible that it is much simpler than it superficially seems, that you can see what they each contribute to the problem and simply average them out with the law of large numbers, just like one can flip enough coins and be confident that they will not all come up tails. For 10 coins the probability of such a thing happening is 1 in 1,000 (and for 20 coins it is 1 in 1,000,000):</p>

<img alt="" src="https://alexboland.s3.amazonaws.com/images/binomial.png" style="width: 624.00px; height: 384.00px">

<p>The problem is that once interactions come into play, we cannot make such assumptions. If one coin coming up heads was more likely to make another coin come up heads, we would have a much different looking distribution, one in which the edges of the chart are much taller and do not simply vanish from sight as they go further out. Even just three interacting parts can create extreme sensitivity to initial conditions, as demonstrated by the meteorologist Edward Lorenz, who created a simulation of the weather with only three variables. At some point, he re-ran the simulation starting somewhere before where he left off, and found that the simulation&rsquo;s trajectory differed wildly from the path of the simulation the previous day, despite the simulation being entirely deterministic. It was only later he realized that the simulation ran on six significant digits but his printouts only showed three of them, and that he had used the three digit figures&nbsp; as inputs when he wanted to re-run the simulation.&nbsp;</p>
<p>In this case, one could make the argument that this is just a simple error of not taking into account everything important, but what if the number of important things is infinite? This is precisely the case of the Three Body Problem: if there are two planetary bodies orbiting one another and therefore acting gravitationally on one another, it is always possible to write a &ldquo;closed-form solution&rdquo;&mdash; an equation with a finite number of parts &mdash; to describe their trajectories, but this is <em>not</em> necessarily the case should there be three such bodies; instead, their interactions may be so critical to one another that any finite amount of precision is not enough to anticipate their long-term outcome.</p>
<p>If just three variables can cause this much trouble, how can we make anything work when there are hundreds or even thousands of them?</p>
<p>Despite all this, pharmaceuticals by and large <em>work</em> in some sense, having effects that at least in the short term would be considered &ldquo;predictable&rdquo; by some reasonable rough definition, and this success always at some point comes down to a substantial amount of trial and error. Even in applications adjacent to physics, in which machines rather than organisms are built and fixed, there is always a miasma of ostensibly &ldquo;blind&rdquo; tinkering ultimately responsible for making things work, with theoretical explanations either outright ignored or amended only after the fact: even after a century of flight, wind tunnels are still in use for testing just about everything that travels through the air, while manufacturers of top-of-the-line microchips can only produce them because of a long chain of seemingly superstitious in-house practices passed down through successive generations of engineers that are themselves inseparable from the material substrate of the factory itself. (India, to this day, is decades behind in the manufacture of microchips despite being at the cutting edge regarding microchip <em>design</em> due to a fire that burned down their facility in 1989.)</p>
<p>In all these cases, it is&nbsp; not simply a matter of utilizing the findings of some fabled &ldquo;pure&rdquo; science as first principles the way one might use an instruction manual to assemble prefabricated furniture, but a process of <em>engineering</em> &mdash; but is this simply &ldquo;theoryless,&rdquo; utilizing nothing more than sheer trial and error guided by the intuition of the scientist? And what does one mean by &ldquo;intuition&rdquo;? Does it just mean the gut feelings of the researcher in question, something predictive in a way that transcends the simplicity of rigid mechanical explanations? It can certainly be very powerful in some cases, such as one retold in Daniel Kahneman&rsquo;s <em>Thinking, Fast and Slow:</em></p>
<p><blockquote>The psychologist Gary Klein tells the story of a team of firefighters that entered a house in which the kitchen was on fire. Soon after they started hosing down the kitchen, the commander heard himself shout, &ldquo;Let&rsquo;s get out of here!&rdquo; without realizing why. The floor collapsed almost immediately after the firefighters escaped. Only after the fact did the commander realize that the fire had been unusually quiet and that his ears had been unusually hot. Together, these impressions prompted what he called a &ldquo;sixth sense of danger.&rdquo; He had no idea what was wrong, but he knew something was wrong. It turned out that the heart of the fire had not been in the kitchen but in the basement beneath where the men had stood.&nbsp;</blockquote></p>
<p>How did he do this without even consciously knowing what he saw? After decades of experience, you see more and more common patterns, and the more familiar you are with them, the fewer cues you need to know what pattern you are seeing. The expertise of chess masters works quite similarly &mdash; they do not need to do exhaustive calculations because they have the experience needed to simplify their reasoning. &nbsp;</p>
<p>One might even start to believe this works for something like picking stocks, but the studies show that this is not the case. At the end of the day, there are few, if any, stable patterns to converge on when it comes to the movements of specific prices. The traders that do reliably make money are ones who build financial instruments that give them some kind of arbitrage through subtle arrangements of bets that do not fully even out, and even they end up having to cut corners by relying on estimates that do not always work out in their favor.</p>
<p>Gut feelings have no special predictive power: past success alone does not guarantee future results, and when they seem to it is often an artifact of survivorship bias, in which one most easily sees the track records of those who are still around to tell the tale. Just like the equations and simulations that attempt to predict the weather and the movements of planets, gut feelings used in the service of prediction are based on models, simplifications of reality&mdash;tacit and explicit alike&mdash;that allow one to do more with less.</p>
<p><h2>Models&nbsp;</h2></p>
<p>The fundamental concept of information theory, a field of applied math developed by Claude Shannon to theorize about reliable communication, is compression. To compress something is to describe it with less data. Some compression is lossy, such as when you take music from a vinyl record and turn it into an MP3 : the basic information is still there, but there is a loss of detail. In cases where no details are truly lost, the compression is lossless, which we will focus on for now. &nbsp;Consider the following string of binary digits:&nbsp;</p>
<p>111111111111111111&nbsp;</p>
<p>And this one:&nbsp;</p>
<p>1010101000010101&nbsp;</p>
<p>Which one of these would take more space to describe? With the former, you can just say &ldquo;repeat the number 1 eighteen&nbsp; times&rdquo; &mdash; but what about&nbsp; the latter? Of course&nbsp; there might be some way to make the description of the former sequence shorter, but you see what I am&nbsp; getting at.&nbsp; A mathematical theory called Kolmogorov complexity deals with exactly this subject by asking, &ldquo;What is the smallest computer program that I can write that would output this?&rdquo;</p>
<p>Such a computer program constitutes a model. A more general way of imagining compression, without having to think about computers, is the concept of a <em>homomorphism</em>: imagine any number of objects that are related to each other in certain ways. To create a homomorphism, you create a new group<sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup> consisting of some (but not necessarily all) of those objects such that the objects in this new group still have the same relationships to one another that they did in the old group. In more general terms, it consists of creating a smaller version of something that has the exact same structure.</p>
<p>A simple example of a homomorphism is the seconds hand on an analog clock: imagine one that ticks every second and one that makes ticks that are twice as big every two seconds. The former makes more fine-grained movements, but they both still tell time accurately. More importantly, you can convert the movements of the more "fine-grained" clock to the movements of the "coarser" clock without losing their structure.&nbsp;</p>
<p>But what do I mean by "structure?&rdquo; In the case of clocks, any two movements of the second hand on a clock can be added into a new movement: in the case of a single tick for every second, adding 10 ticks to 30 ticks is the equivalent of advancing 40 ticks, and adding 30 ticks to 50 ticks is the equivalent of advancing 20 ticks since the hand goes in a cycle of 60 seconds. I can do the same for the other more "coarse" clock, except in this case the hand goes in a cycle of 30 seconds, which changes the rules of addition, but only superficially, because if I take any two movements from the "coarse" clock and add them together before switching to the "fine" clock, it will be exactly the same as converting both movements to the measurements of the "fine" clock and then adding them together there.</p>
<p>This property is also known as congruence, which may be familiar from grade school geometry. &nbsp;A model that is congruent with its subject is one that is capable of anticipating its behavior. Why is this? Consider how you can tell how much time will pass with even a relatively "coarse" clock: it will not&nbsp; get out of sync with the more "fine" clock (if we disregard the effects of relativity for the moment). Similarly, a model that treats a subject in a more "coarse" manner will not fall out of sync with the subject it is modeling so long as there is congruence. In this case, it would not matter which of the two paths are chosen to get from the top-left corner to the bottom-right of the diagram below:&nbsp;</p>

<img alt="" src="https://alexboland.s3.amazonaws.com/images/congruence.jpg" style="width: 616.85px; height: 363.35px;">

<p>Here, the downward arrow on the right is essentially a prediction about how the "actual" thing on the top-left will change. If the prediction were inaccurate, then taking the left-hand arrow (watching the object change) and then applying the bottom arrow (encoding the changed object) would provide a different result than first taking the top arrow (encoding the original object into your model) and then using the model on the encoding (applying the model's prediction).&nbsp; If, on the other hand, the predictions reliably come true, then there is congruence between the model and the subject being modeled where, just like the example of the clocks, the "finer" phenomena can be converted into the "coarser" phenomena and then computed and the results will be the same as working within the former and only in the end encoding it into the latter. In category theory, a diagram that achieves this lack of path-dependence is said to commute, and an encoding, such as the one shown above, is called a <em>functor</em><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup>.</p>
<p>Of course the problem is that a model can appear to be congruent until it doesn&rsquo;t&mdash;like when Lorenz found out that the version of his simulation that only used three digits after the decimal point was not&nbsp; congruent with the one that used six, or when the gambler suddenly &ldquo;loses his touch.&rdquo;&nbsp;</p>
<p><h2>Falsification</h2></p>
<p>Given this fallibility of models as explanations, how can we hope to discover anything? Or perhaps more pertinently, how do we come up with a model to begin with? From a certain simplified idea of the scientific method, one could simply find a single variable to tweak and make sure all other variables are unchanged and then proceed to see what happens, isolating the possibility of any confounders. Of course, if changing just one variable does not yield anything noticeable, then there is the option to test variables in pairs, and from there in triplets, and so on. This sounds ideal until one considers just how many iterations it would take: if one were to do this with just 10 variables, each of which with a possible value of either &ldquo;true&rdquo; or &ldquo;false&rdquo; it would take 1,000 different tests, a number that with just 20 variables would go up to 1,000,000 such tests, and continue to exponentially increase from there&mdash;all of this assuming that the variables are already just lying around in an unambiguously defined way.</p>
<p>Given that there are only so many hours and resources to run experiments, the only option is to limit experiments to configurations of variables that seem plausible, which means determining some underlying factor that is not superficially observable, which means re-introducing models &mdash; models that are ultimately conjured up from some kind of intuition, as explanatory frameworks, even if they are just hypotheses. Given that a model can be congruent with its subject matter for any amount of time until it suddenly is not, there is no way to strictly prove its correctness. But it only takes one counter-example to prove a model wrong, which was Karl Popper&rsquo;s idea of how progress is achieved. A model achieves a basic kind of plausibility when the experiment can be reproduced, but if it is&nbsp; wrong it will inevitably be culled by some experiment that demonstrates a counterexample. The idea is very intuitive, but it runs into difficulties in the real world: any number of errant things could have messed up the experiment and created a false positive; maybe the&nbsp; lens is smudged or someone wrote down the wrong data. To his credit, Popper accounted for this:&nbsp;</p>
<p><em>"Non-reproducible single occurrences are of no significance to science. Thus a few stray basic statements contradicting a theory will hardly induce us to reject it as falsified. We shall take it as falsified only if we discover a reproducible effect which refutes the theory."&nbsp;</em></p>
<p>There is just one issue: if a counter-example requires reproducibility in the same manner as a model, then there is no longer any fundamental asymmetry between proving and disproving a rule. This damning logical flaw comes not so much from a wrong answer as it does from asking the wrong question: trial and error, and for that matter engineering, is not about getting predictions right or wrong, let alone establishing a claim as universally True. It is about doing what works with respect to the matter at hand, which is not necessarily something fixed and immutable or reliable precisely one hundred percent of the time or applicable to one hundred percent of scenarios. Relativity may have superseded Newtonian mechanics as an explanation of motion through time and space, but given that Newtonian mechanics is sufficient for the vast majority of engineering tasks when dealing with the relatively slow speeds we encounter on Earth, it would be a stretch to suggest that it has been invalidated, even if from a foundational viewpoint it is neither the Truth nor what one would generally utilize if they wanted to make new discoveries in physics.</p>
<p>In other words, engineering relies on the <em>local</em> validity of an idea: so long as the idea is effective and reliable within a certain domain under certain conditions with the help of certain tweaks or even a selective application of the idea it is without any further qualification &ldquo;true&rdquo; with respect to such local conditions. Under these circumstances, falsification is not a matter of a single counterexample or even a reproducible counterexample because the counterexample is not&nbsp; necessarily relevant to how the idea is employed. On the other hand, to use an extreme example, if a certain novel method of building a bridge is used and the bridge later collapses under far less weight than estimated, then unless somebody really messed up in some other way, this hypothetical method of construction would be, for all intents and purposes, falsified.</p>
<p>But what about the case of Science, where one could argue that the goal is to find <em>global</em> validity, or perhaps even Truth? How does one do this when there is no straightforward instrumental problem to decide what &ldquo;works?&rdquo; More importantly, however, why is there any reason to suppose that congruence between a model and some arbitrary number of observations constitutes the truth of this model as an explanation? For one thing, what makes the <em>observations</em> valid? Even beyond the concerns about intermittent errors that forced Popper to backpedal, how does one even establish that a number on a machine represents what others claim it to?</p>
<p>From a practical point of view, there are a great many answers, but all of them are answers about the efficacy of science in a purely instrumental way. To rely on such answers would be to simply equate science with engineering and in doing so throw out the baby with the bathwater by abandoning any faith not only in achieving anything more universal, but also in the ability to achieve agency through any kind of positively defined knowledge. But in order to find an answer that avoids this pitfall, one has to reconsider concepts such as &ldquo;observation&rdquo; and &ldquo;intuition&rdquo; from a more pragmatic standpoint and reconstruct the idea of science not as a passive correspondence between conjecture and reality from which engineering creates applications but as an active elaboration of affordances that begins but does not end with engineering.</p>
<p><h2>Ecology</h2></p>
<p>As noted earlier, even if given a set of well-defined variables, there are in many cases just too many permutations to test them all, and so when figuring out which ones to test one must narrow their choices down along the assumptions of some kind of model. But how does one come up with a model? There is not necessarily some bucket one can just stick their hand in, and to say &ldquo;intuition&rdquo; without any further qualifications is to suggest more or less the same idea. In such cases, one is left to <em>construct</em> a model, which means picking relevant attributes, which themselves do not exist ready-made either: sure, in a practical common-sense way one could claim they are often readily apparent, but to define them <em>formally</em> requires defining attributes in terms of other attributes, which themselves are defined in terms of even more attributes, all of which constitutes a potentially infinite web of intensionally defined concepts.</p>
<p>But all this leaves the question of how any of this is grounded in the physical world in which this takes place: if something abstract gets its definitions entirely from something else abstract, then how can it say anything about the real world? One could propose, loosely along the lines of Logical Positivism, that all such formal entities and their interminable web of interdependencies could be ultimately traced back to <em>sense data</em>: sensory stimuli that would then form the building blocks of abstractions which could then themselves form the building blocks of other such abstractions. Given this, one could then receive and compute on such sensory information in order to determine their next action. Unfortunately, this is not how sensation works.</p>
<p>Coherent sensory information often requires more than passive reception. For example, consider the problem of robotic vision. One may want to have their field of view at any given time represented by a two-dimensional grid of pixels, with the color of the pixels determined by how the light is hitting their eyes. Do they then &ldquo;see&rdquo; a table? Not necessarily: just like there are many models that could corroborate a given set of observations, there are many arrangements of objects that could create the same pattern of light hitting the eyes at that angle. Instead, in order to infer that this is a table, the robot has to move around and look at it from many angles, performing a process of falsification similar to the falsification of models.<sup><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup> Humans do this congenitally on a multitude of levels&ndash;even the human eye makes rapid movements every second in order to paint a more complete picture of their immediate point of view.</p>
<p>Observation is therefore a process of <em>interaction</em> between an actor and their environment. In this case, the actor, whether human or animal or robot, moves around the object, shedding possibilities of what it could be, until it finds what is <em>invariant</em> &mdash; that is, what one still arrives at independently of how they got there; just like how a valid model (if one looks back to the earlier diagram) has the same destination whether one lets the experiment run and encodes the result or encodes the initial setup and runs the model on the encoding. The object ultimately inferred did not simply manifest itself in a simple stimulus or even a composite of such stimuli but in an equilibrium established by a kind of tinkering.</p>
<p>And because this is the case, there is no reason why the observer has to be entirely passive with regard to the object in question. A spider&rsquo;s own observations about prey explicitly involve altering the environment by spinning a web and doing so in a certain way: if they want to be alerted to smaller prey they will make the web more taut, or if they want to save their energy and only catch larger prey, they will allow some slack.<sup><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup> This is a simple and relatively passive example of a larger principle that applies to all lifeforms: an organism&rsquo;s &ldquo;environment&rdquo; is never a passive pre-determined stage but a constructed habitat that in turn determines the nature and significance of their own observations. In other words, beings construct <em>their own</em> semantics, and while these semantics are in some sense &ldquo;relative,&rdquo; they are nonetheless no less real than anything else, as they are both dependent on and depended on by the affordances within their environment.</p>
<p>This idea of organisms crafting their own ontologies originates with the biologist Jakob Johann von Uexkull, who named this concept the <em>umwelt</em>, the world as &ldquo;experienced&rdquo; by a specific organism. But this iterative and relational process by which organisms come to define their own concept of the outside world also provides a starting point for how to understand how a model and that which the model is tested against can come into being. For an engineer, if something can be tampered with in order to make it behave in accordance with an easy-to-understand model, then doing so is a completely valid and reasonable move; and while one may not be able to directly perceive a subatomic particle, an entire web of inference for &ldquo;observing&rdquo; them was established not simply by reasoning abstractly up a chain of propositions that begins with sensory stimuli, but through an active process of social and material terraforming involving increasingly sophisticated facilities and equipment evolving in tandem with tacit practices and formal guidelines passed down through channels of apprenticeship.</p>
<p>From the most basic correlations of stimulus and response to billion dollar facilities dealing in the most ineffable of abstractions, the formation of umwelten is the basis of symbolic grounding, and by extension of epistemology. Such a process, however, can only take place through a &ldquo;chicken-and-egg&rdquo; recursion by which an already existing gestalt further articulates itself. This continual capacity for elaboration is at the heart of what allows falsification to happen in science even without there necessarily being an externally imposed engineering problem or material need or any specific rule involving statistical significance or reproducibility (even if these are often things one wants to have in particular cases).&nbsp; But in order to understand how such a thing is possible without resulting in a mess of arbitrary tautologies detached from the rest of the world we will have to start <em>in medias res</em>.</p>
<p><h2>Conflict</h2></p>
<p>There is, at the heart of every engineering problem, a <em>conflict</em>. A bridge must be built or repaired, a new technology is desired or needed, a novel complication must be accounted for in what would otherwise be a routine task, or perhaps somebody just wants to make some money.&nbsp; From a first-order point of view, scientific impasses and their ensuing breakthroughs are no different: prior to the discovery of relativity, astronomers found themselves unable to account for subtle deviations in the movements of certain planets from what they would expect based on the calculations of classical mechanics, and to go back to my earlier qualification about synchronized clocks, clocks were themselves subtly falling out of sync for seemingly inexplicable reasons.&nbsp;&nbsp;</p>
<p>But while no engineer acting in good faith would want to see a bridge collapse or a project run over budget, the last thing a scientist would want is for their predictions to always be <em>right</em>: As of today, billions of dollars have been spent on machines to smash atoms apart, only for the predictions of the &ldquo;standard model&rdquo; of particle physics to be further corroborated. This may sound like an unalloyed good, but while the Standard Model is not in <em>contradiction </em>with any of the observations, this also means that it hasn&rsquo;t been falsified in a way that would give some direction to addressing the questions it still can&rsquo;t answer.&nbsp;</p>
<p>Science is therefore not orthogonal to engineering, nor is engineering simply a collection of concrete manifestations of ostensibly ethereal abstractions uncovered by science. Engineering is a process by which a pocket of tractability is created by any means necessary (including but not limited to concepts and models borrowed from &ldquo;pure&rdquo; science, or from anywhere else for that matter), fundamentally no different from the way in which even simple sensory experiences are the result of an actively maintained equilibrium between a subject and its environment. In the case of scientific research, however, there exist such problems <em>endogenous</em> to the overall endeavor, not simply appearing as externally imposed obstacles that block otherwise effectual scientific progress, but manifesting as part and parcel of a larger umwelt continually constructed and shaped by its participants. That is to say these problems are neither exclusively &ldquo;real&rdquo; in the sense of being unmediated material facts nor &ldquo;ideal&rdquo; in the sense of being otherwise arbitrary symbolic puzzles, but rather problems that are every bit as formal as they are material insofar that they are continually mediated through the <em>technology</em> of its umwelt.</p>
<p>This technology, specific to the logic and purposes of its respective scientific enterprise, is the material and social means by which the nebulous interactions between the umwelt and the outside world (along with any messy interactions happening within) are individuated into a syntax of formally defined models and observations sufficiently precise and stable to allow further reasoning and further development of this same technology&mdash;this syntax being the underlying <em>theory</em> by which all such abstractions and empirical expressions are defined. Any such problems with said technology could hypothetically be a &ldquo;software&rdquo; issue concerning a contradiction amongst the formal inputs and outputs of the machines or a &ldquo;hardware&rdquo; issue in which some instrument breaks down or in some way fails to provide an output when needed (which could also be a &ldquo;software&rdquo; issue when one considers Alan Turing&rsquo;s <em>Halting Problem</em>).&nbsp; In either case, any such error ultimately presents itself as a formally defined vacuum to be filled, whether in the form of a logical contradiction to be resolved or a question without a definite logical answer.</p>
<p>The question of what is done to fill such vacuums is addressed by the philosopher of science Thomas Kuhn in his seminal work, <em>The Structure of Scientific Revolutions</em>. In his vocabulary, what I have been calling the umwelt of a scientific pursuit is known as a <em>scientific paradigm</em>: the formal models, tacit practices, physical infrastructure, personnel, educational curricula, etc. that together embody an active and ever-changing scientific pursuit. Notably, nothing about a scientific paradigm has to be clean or straightforward or even logically consistent from the point of view of an outsider: the education of a physicist may involve all sorts of idealized and sanitized textbook theories and corresponding problem sets that are known not to correspond with what happens in practice, while experiments are not necessarily falsified according to hard and fast rules but may in part be dependent on the intuition of an experienced member or a collective, tacit understanding of when to look the other way. Relativity, for example, while now considered undeniably true (or at least &ldquo;more true&rdquo; than classical mechanics), did not require an overwhelming amount of data to be accepted, and even when it was tested there were early instances ostensibly falsifying it.</p>
<p>This very messiness is in fact absolutely key to the vitality of a scientific paradigm, as scientific research is always in some sense about the resolution of its own logical paradoxes, which can in turn be resolved in one of two ways: First, one can look for missed details or slight miscalibrations and misunderstandings in order to deem an apparent inconsistency nothing more than a quotidian mistake. At most times, this is how science proceeds: there are all sorts of relatively rote &ldquo;puzzles&rdquo; to be solved by use of existing models or perhaps by tweaking such a model just a little bit, diligently filling in the holes of a cogent but craggy epistemic landscape.&nbsp; The meticulous day-to-day effort to solve such puzzles is what Kuhn calls <em>normal science</em>.</p>
<p>But given this tolerance for dissonance and the fact that one could always hypothetically shift the goal-posts in order to preserve some pretense of &ldquo;truth,&rdquo; what is the impetus for falsification? One might consider a market realist approach in which if scientists cannot come up with a useful technological breakthrough then they might not get funded, but given how often science does not in fact quickly and predictably yield narrow instrumental benefits and often requires a great deal of patience and uncertainty before bearing fruit for the rest of us, this is hardly a satisfying explanation. Furthermore, it fails to consider <em>what</em> scientists would choose to falsify in the face of apparent stagnation, even if they felt that <em>something</em> needed to be done differently to make more breakthroughs. This can only be answered through the holistic understanding of someone intimately attuned with the paradigm&rsquo;s technological apparatus&mdash;a type of engineer who works within a scientific paradigm, also known as a scientist.</p>
<p>But while a paradigm&rsquo;s fundamental criteria for falsification may not be anything defined by some external instrumental standard, it is nonetheless a question of efficacy with respect to its own reflexively generated dilemmas. A paradigm&rsquo;s very concept of &ldquo;forward&rdquo; is based on such conflicts, as they provide the context for any and all action taken within it, and for this same reason said conflicts must be actionable in a way that opens up further conflicts that are themselves actionable. Simply put, any such resolution must allow a paradigm to <em>do</em> more, not only by creating new formal questions to answer, but also, coextensively, effecting new material affordances.</p>
<p>When the paradigm is in a state that Kuhn calls <em>normal science</em>, in which scientists tinker around the edges, solving routine puzzles in order to further calibrate the extant theory and practice of the paradigm, there may be plenty of apparent contradictions and shortcomings: models that are accepted as valid but logically contradict certain observations or exist in contradiction with another such model, or perhaps attempts at observations that prove inconclusive. These may be innocuous misunderstandings or technical hiccups that are straightforwardly resolvable with some modifications to models, definitions, or instrumentation.&nbsp; In other cases they may be inconsequential enough with regard to current research that they can simply be overlooked without compromising the momentum of the paradigm.</p>
<p>Sometimes, however, one may encounter a Contradiction, in which there is something much more fundamentally wrong that severely impedes further progress. While such contradictions are still the same in the sense that they come from a formal mismatch between model and factum, they are not simple <em>empirical</em> contradictions that can be resolved by tweaking formal definitions and assumptions or technical minutiae until the feedback is mediated in a way that maintains congruence with the system. Instead, this is a kind that materially expresses itself as a deep gridlock, betraying a deeper inconsistency foundational to the theory, in many ways resembling the kind of self-contradictions that bring about the collapse of economic orders spoken of in Karl Marx&rsquo;s dialectical materialism, and just as these crises foment revolutions in the realm of politics, a crisis within a scientific paradigm initiates a revolution of its own as the paradigm enters into a phase that Kuhn calls <em>extraordinary science</em>, in which routine maneuvers no longer suffice and must instead yield to decisive acts of imagination.</p>
<p><h2>Incompleteness</h2></p>
<p>But how can a logical contradiction force an entire paradigm to a halt? Is it not always possible to shift the goal-posts, whether through formal definitions or the mechanics and interpretation of instruments, as suggested before? One may of course advocate at this point for some degree of common sense or good faith, and this is on some level a valid answer, except that these notions are <em>themselves</em> paradigm-specific, which is why physicists have indeed practiced sensibility in many ways when deciding how to go about interpreting results or running experiments but have nothing to prove to outsiders who make claims about relativity or quantum physics or string theory being &ldquo;nonsense&rdquo; because it doesn&rsquo;t fit <em>their</em> idea of common sense.&nbsp;&nbsp;</p>
<p>This is no guarantee, however, that any kind of &ldquo;common sense&rdquo; at all would suffice as a watchdog for semantic drift within a paradigm, as the very notion of intuition, the conceptual terrain through which our thoughts are mediated, one that in the context of science is not merely something exclusive to a single scientist&rsquo;s subjectivity but embedded in the concepts and affordances of the paradigm they are working in.&nbsp; What is needed is to understand how certain contradictions can <em>only</em> be resolved through the active creation of a new concept, which is to say that they are indicative of some proposition for which no answer can be logically formulated. Fortunately, there was a thinker who worked with exactly this kind of problem in the realm of pure logic: Kurt G&ouml;del, the father of the infamous Incompleteness Theorems, which I will summarize and cherry-pick in a way fitting for our purposes:</p>
<p>To begin, a <em>theory</em> in mathematics, similar to my own definition of a theory in a scientific paradigm, is a syntax defined by a collection of axioms and syntactic rules for combining and transforming propositions. Now, assuming that the theory does <em>not</em> have any contradictions (i.e. the axioms cannot be used to reach two distinct answers for the same formally defined question), it may, depending on the nature of the theory in question, contain a proposition that is <em>undecidable</em>: while the proposition as a question can be derived using the provided axioms and transformation rules, an answer to said question cannot.</p>
<p>The requirement that the theory have no contradictions is key: it tells us that a theory of this kind cannot be both self-consistent and complete, where complete means that all questions that can be assembled using the formal language of the theory have an answer that can be derived from the mechanical application of axioms and transformation rules. This means that if you simply force an answer to one of these undecidable propositions, it will become possible to derive the exact opposite answer to that same question, thus making the system logically inconsistent.</p>
<p>But what kind of theory is he talking about that&rsquo;s prone to this kind of impasse? Potentially many, but in the first of his two incompleteness theorems, G&ouml;del is referring to a mathematical theory capable of doing arithmetic. In particular, the <em>First Incompleteness Theorem</em> states that no mathematical theory capable of performing a certain level of basic arithmetic can be used to mechanically derive all true statements about itself, as one can effectively model such a theory using a special system of arithmetic on prime numbers and to construct a statement that is self-contradictory and therefore cannot be proven either true or false without destroying the consistency of the theory.</p>
<p>A related problem formulated by Alan Turing known as the <em>Halting Problem</em> shows a similar issue by stating that for all turing machines (i.e. computers), there exists at least one program such that if the turing machine were to run that program, it could not determine in advance whether its run of that program will ever terminate (i.e. whether the program qua proposition is decidable).&nbsp; This is due to similar issues of reflexivity as the ones seen in G&ouml;del&rsquo;s First Incompleteness Theorem, as one can always create a sufficiently contrarian program that does the opposite of whatever such a turing machine would anticipate.</p>
<p>The factor common to both of these cases is that should a system be capable enough of self-reflection that it can be fed a logical paradox, then its own formal rules of inference will never be enough to account for every question its language could imply. Given such an undecidable question, by which no rules of inference can be used to get a meaningful answer (one could get a <em>meaningless</em> answer by getting multiple contradicting answers), the only option left is to invent a <em>new</em> axiom to fill in the gaps. But even if one fills in all such holes using such an axiom, this same fundamental incompleteness will ensure that these axioms simply create new holes of their own as they expand the range of questions to be asked.</p>
<p>The import of these undecidable propositions is that they are ways to effectively fall off the edge of a map. Imagine a mathematical theory complete with the natural numbers, addition and subtraction, but no negatives. Addition can be trivially applied to any two numbers and you will&nbsp; end up with a number that remains in the theory, or in other words, the theory is <em>closed</em> under addition. On the other hand, if you try subtracting a larger number from a smaller one, there is no answer, and therefore no closure. This is not a mathematical framework sophisticated enough for G&ouml;del&rsquo;s First Incompleteness Theorem to apply, but it is demonstrative of the way in which such a gap necessarily requires something new from outside, namely the concept of negative numbers.</p>
<p>This is also not simply a symbolic concern: these concepts are brought into being not out of idle thought experiments on arbitrary symbols, but because they represent the need to do more things, as they are themselves the symbolic expression of real material affordances. To use a comically simplistic example, the concept of debt, or any kind of deficit at all, is impossible without negative numbers, and so if one wishes to create a system where one can purchase things on credit, one will inevitably invent a concept along the lines of negative numbers in order to accommodate the &ldquo;real world&rdquo; action of buying things on credit. Similarly, negative numbers would be a meaningless language game in this paradigm so long as the possibility of using credit is not actualized.</p>
<p>Science is a similarly self-reflecting process, equipped with this same incompleteness by virtue of operational closures sufficiently powerful to express propositions beyond its capacity to contain them. These operational closures are in effect new concepts by which a scientific paradigm engages with the world, but with each new concept comes new questions to be formulated <em>about</em> those very concepts. In the simple example regarding negative numbers, one can note that the invention of negative numbers (in a more complex context than my toy example) created the hypothetical of what would happen if one took the square root of a negative number, in turn leading to the invention of the imaginary numbers (which themselves have been instrumental in the advancement of telecommunications technologies).&nbsp;</p>
<p>Such undecidabilities, as in the case of questions regarding particle physics left unanswered by the Standard Model, may leave glaring holes that would require trying out new models, but in other cases these same undecidable propositions may manifest as crisis-causing Contradictions, such as the crisis leading to the development of relativity, which prevent any further progress and in doing so mirror a foundational deficit in which there are no further moves that do not simply loop back into the same stalemate. That is to say, a contradiction is a Contradiction (a foundational contradiction) when business as usual has exhausted all &ldquo;possible&rdquo;, but not all imaginable, breakthroughs.</p>
<p>To create such an operational closure is therefore to invent new moves in the material world of the paradigm. Given that closures are such that one can apply an operator, i.e. <em>do</em> something, and always get a result that is within the bounds of the theory, and given that a scientific theory is a representation of the formal conceptual structure induced by the paradigm&rsquo;s technical mediation, one can suppose that this is effectively the creation of novel <em>intuition</em>, a new kind of space in which the operational closure exists in its own right as a new way to move through concepts; and given that it is a change to the technical mediation of the &ldquo;outside&rdquo; world, an entirely new kind of <em>matter</em> not fully reducible to the intertwined process of imagination and engineering that spawned it.</p>
<p>This phase of creating new operational closures and fomenting a rapid and decisive change to a scientific paradigm is the way in which scientific revolutions are fulfilled. The invention of such operational closures, being the coextensive creation of concepts and affordances alike and therefore both formal and material in its nature, is simultaneously an act of imagination and manufacture, both poetic and technological in its creation of a terrain neither predetermined nor even enumerable as a &ldquo;possibility&rdquo; within some distribution of probabilities but entirely contingent with respect to what could be described from the standpoint of the earlier paradigm.</p>
<p>This inherent porousness, inexorably demanding creative leaps and new material to continue living and breathing, provides an ever-changing presentation of ports that allow theorist and practitioner alike to plug in novel concepts and technologies in order to relentlessly adapt what would otherwise be arbitrary abstractions to the inexorable flux of otherwise unmediated reality.&nbsp; Incompleteness is therefore what allows us to create propositions that actually represent things; without it, no symbol could mean anything at all.</p>
<p><h2>Narrative</h2></p>
<p>Given the relentless existence of these open-ended questions by which a paradigm is fundamentally exposed, one cannot solve all questions posed by its theory by running an algorithm on it. This means that any such theory is first and foremost a text to be interpreted, with each act of interpretation being essentially &ldquo;analog&rdquo; insofar that these acts are by definition not syntactically expressible by that theory. This is to say that by definition no two interpretive acts can truly be the same (at least from the point of the view of said theory) as there would exist no formal procedure by which to evaluate their equivalence <em>vis a vis</em> one another.<sup><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup></p>
<p>Interpretation in this sense is not some passive exercise of summarization confined to some solipsistic definition of &ldquo;subjectivity,&rdquo; but a constructive act inseparable from its material utilizations and consequences. In the case of science, this is most visible in times of crisis where dramatic leaps of imagination must be employed before research can once again smoothly progress, but even periods of normal science are not exempt from this irreducible need for initiative: as noted earlier, the utilization of observations and models, while mediated through their paradigm&rsquo;s formal machinery, is never a purely symbolic operation, as scientists must always in some capacity continually negotiate the tacit details of when and how to use them and when to look the other way should a contradiction occur.</p>
<p>But this indissoluble need to exercise judgment does not mean that science is simply some unpredictable free-for-all: to formulate and operationalize new scientific concepts is to engage in a laborious process of integration, obeying countless necessary constraints and working around others only by understanding their subtleties enough that one can subvert them the same way that a practitioner of judo must position his body in precisely the right way to turn his opponent&rsquo;s own momentum against them. Should an act of interpretation not take into account such subtleties and treat them with the respect that they demand, it will not properly fill the gap that needs to be filled, instead inaugurating much more damning contradictions that would only further compromise the dynamism of the paradigm.</p>
<p>The evolution of a scientific paradigm therefore, while fundamentally contingent, is not arbitrary.&nbsp; While there is no unambiguous criteria for saying whether one valid interpretation is &ldquo;better&rdquo; than another, some interpretations are for completely pragmatic reasons <em>not</em> valid. Furthermore, the fact that any such valid interpretation supplements the theory in such a way that it contains its predecessor (e.g. Newtonian mechanics being an effective approximation of relativistic mechanics under the condition that everything is moving slowly enough), a scientific paradigm necessarily provisions its development with a certain kind of telos.&nbsp;&nbsp;</p>
<p>This telos, while not characteristic of an unequivocal endgame, applies in the sense that at any given point a paradigm embodies a narrative<em> </em>by which it contextualizes and explicates the significance of the events leading up to its current state and the possibilities ahead. Notably, a narrative in this sense is not a mere succession or summary of events, nor is it a specific interpretation, but a gestalt that facilitates interpretation, effectively a shared ecology containing a text to be traversed and actors to do so, embedded in a material environment in which all actors find themselves.</p>
<p>Put another way, every narrative is specific to the interaction between actors and text within a specific environment. A single person reading a novel is a single actor interpreting a text as they read it, bringing settings and characters to life using whatever prior experiences and intuitions they have to flesh out the details not explicitly stated by the text. A performance of a play, however, is not only interpreted in its own way by each audience member within the confines of their own subjectivity (and sometimes cues from other audience members), but itself built from an active process of interpretation engaged in by the actors themselves, all of whom participate in a single narrative of their own ongoing construction as they cooperatively interpret the script through their actions onstage.</p>
<p>Just like the example of a scientific endeavor, every narrative progresses by building on itself, creating increasing constraints as time goes by, but at the same time building up to a fundamentally richer notion of Possibility: just as a scientific paradigm finds profound truths when it exhausts all possible moves on the board and compels a revolutionary concept, the climax of a story is exactly where the larger problematic is fully defined and where for this exact same reason whatever resolution ensues has the most potential for shaping the entire meaning of everything that came before.</p>
<p>From this vantage point, it is much easier to understand the importance of theory, even in cases where its presuppositions and speculations have not always measured up to the instrumental veracity of physics: theory is first and foremost a matter of hermeneutics, of how one creates for themselves and others an umwelt, a narrative, to live by and expand their capacity for action. This capacity for action cannot be quantified by some artificially imposed metric or judged by some contrived rule, but only understood from within the narrative by which it traverses its own unearthed affordances. Such hypostases may yet radically change, or even essentially die at some point as others replace them, as external pressures can never be truly predicted and people&rsquo;s needs and dreams never stay fixed for very long. But until that time comes, there is only one way to find out how the story ends.</p>

<hr class="c7"><div><p class="c12"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c6">&nbsp;By &ldquo;group&rdquo; I do in fact mean a mathematical group, but you don&rsquo;t need to know what that is in order to follow along: just imagine a set of things and roll with it from there.</span></p></div><div><p class="c12"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c6">&nbsp;Technically this diagram represents a </span><span class="c6 c4">natural transformation</span><span class="c6">&nbsp;between two functors (so long as one thinks of the left-hand side as also being a functor, perhaps itself an encoding of something else in the way that, as I will elaborate on throughout the essay, observations are themselves always explicated with some </span><span class="c6">kind of formalism). &nbsp;The commuting diagram is not a strict requirement for the encoding on the right to be a functor, </span><span class="c6">but </span><span class="c6">the actual requirements for functors are why natural transformations are possible.</span></p></div><div><p class="c0"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span>&nbsp;Laurence Shapiro, Embodied Cognition (New Problems of Philosophy) </span></p></div><div><p class="c12"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c6">&nbsp;</span><span>https://link.springer.com/article/10.1007/s10071-017-1069-7</span></p></div><div><p class="c12"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c6">&nbsp;</span><span>Whether &ldquo;reality&rdquo; is &ldquo;continuous&rdquo; (&ldquo;analog&rdquo;) or &ldquo;discrete&rdquo; (&ldquo;digital&rdquo;) in some absolute sense is a category error in my opinion insofar that it assumes an inaccessible &ldquo;view from nowhere,&rdquo; but ultimately irrelevant when considered from this essay&rsquo;s point of view.</span></p></div>

</div>
</body>
</html>