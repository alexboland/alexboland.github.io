<html>
<head>
	<link href="https://fonts.googleapis.com/css?family=Lora|Montserrat|Raleway&display=swap" rel="stylesheet">
	<link rel = "stylesheet"
   type = "text/css"
   href = "../../style.css" />
</head>
<body class="book">
	<h1>Prototypes and Syntax</h1>

	<i><p>"No experience would count as grounds for revising, for example, that 5 + 7 = 12. Were we to add up 5 things and 7 things and get 13 things, we would recount. Should we still, after repeated recounting, get 13 things we would assume that one of the 12 things had split or that we were seeing double or dreaming or even going mad.  The truth is that 5 + 7 = 12 is used to evaluate counting experiences, not the other way around."</p>

	Rebecca Goldstein, Incompleteness</i>

	<h2>There's No Such Thing as First Principles</h2>

	<p>In light of the above quote, the basic laws of addition do indeed seem to be first principles: the fact that "believing" in them is essential to being able to do science in the first place would seem to say as much.  But if something is a first principle, that would mean that it is not in any way up for any kind of "debate".  In the usual sense, this is true: nobody seriously puts 5+7=12 up for debate in the normal sense of the word.  You can philosophize about it all you want, but every time you engage in a monetary transaction or measure a piece of wood or make sure you packed enough sandwiches for the family or did work in the laboratory, you're demonstrating that you in fact do not doubt the truth of basic arithmetic.</p>

	<p>All that being said, there are places where these rules are not fully taken for granted: amongst theoretical mathematicians, for example, there are times where something like 5+7=12 is not assumed but instead derived from a set of even simpler rules known as Peano Arithmetic.  I will not go into detail here but the axioms of Peano Arithmetic only assume the existence of the number 0 and the ability to generate other numbers by defining a "successor" number, such that 1 is the successor of 0, and 2 is the successor of the successor of 0 and so on.  With a few rules defined on successors and the number 0, it's possible to prove all the basic laws of addition and multiplication on whole numbers.  So these truths are not necessarily <i>first</i> principles as one can come up with something prior to them.</p>

	<p>One may of course ask what the purpose of this is, not true only in a pedantic sense?  Consider negative numbers: these do not come from our everyday experience, you cannot see a negative number of cows or rocks.  On the other hand, the concept clealry has some import: without negative numbers, it would be impossible to balance financial transactions or do all sorts of similar things, and this even extends into the physical world where contraptions such as imaginary numbers are required for the equations that give us things like telecommunication.  People were able to come up with these ideas despite being "self evident" because of the ability to further generalize these basic rules by creating an even simpler set of assumptions.</p>

	<p>But there's no way that the idea of negative numbers could simply be posited ex-nihilo without first encountering the limits of the natural numbers.  We learn the natural numbers and their operations through a combination of experience and our parents and schoolteachers explaining addition and subtraction to us by telling us about what reliably happens when we put things together and take things away.  Negative numbers are simply a result of extending the logic of this story: is it possible to take away 4 rocks if there are only 3 to begin with?  Maybe not in the physical world, but what if you promise someone you'll give them 4 rocks because you know you'll have one more by the end of the day?  Only then do you look for "more prior" principles to expand the possible things you can do.</p>

	<h2>From Prototypes To Principles</h2>

	<p>The acts of counting, adding and subtracting positive numbers become the <i>prototype</i> for the concept of negative numbers.  The concept only exists because the question of "what happens when you subtract more than you begin with?" cries out for an answer, first quietly, when it's still only philosophical, but then gets louder and louder as the question takes on increasingly material and practical motivations.</p>

	<p>But couldn't a framework with negative numbers come out of nowhere anyway?  Surely there is nothing stopping one from creating these symbolic rules and these symbolic rules being consistent regardless of what led up to their creation.  Yes, this is absolutely true, but the problem is that symbolic rules alone do not mean anything.  I could create any number of arbitrary systems of propositions with these or that rules of inference, but they can only have any meaning if they actually *stand* for something, and not just because I said they stand for something but because people can actually find some kind of relevant congruence.  In other words, one must demonstrate some kind of <i>semantics</i>.</p>

	<p>And if you were to go back to prehistoric times and teach cavemen numbers, would you be able to make them understand what negative numbers are?  What would be the basis for accepting the idea of negative 2?  You would not be able to show them an example, because there's not yet a relevant situation that one could clearly point to; they do not yet have currency or any modern notion of debt (as opposed to ritualistic ideas of debt, which are very old).  The only way that negative numbers could become meaningful to them would be to first teach them the natural numbers and then let them ask for themselves "what happens if you take away more than you have?"; which would itself remain a merely metaphysical issue until they gradually learn on their own what such a concept would allow them to <i>do</i>.</p>

	<p>To put it another way: for negative numbers to exist as an actual general principle and not just arbitrary manipulation of logical propositions, they have to have something to generalize.  Generalizing is also not by any means an arbitrary act: the purpose of a general principle is closure.  What do I mean by this?  I mean that if you don't have a concept of negative numbers, subtraction is not an operation that is closed--if you subtract 4 from 3 you simply fall of the edge of the map and can't get back on it.  By introducing negative numbers, anything subtracted from anything else keeps you within the syntax of arithmetic.  The motivation for closing a system is itself pretty simple: the more you can guarantee that some representation of reality is syntactically closed, the more you're allowed to just mechanically crunch the symbols instead of having to consciously worry about whether what you're doing makes any sense.</p>

	<p>This logic does not just apply to an individual trying to save labor but to systems of all scales as well as to the production of more advanced knowledge in general.  The degree to which a person or an institution or a society or anything else can rely on purely syntactic operations is the degree to which it can behave in a way that embodies the logic of these operations and therefore the degree to which such behavior can be composed into more sophisticated <a href="enactments_kernels.html">enactments</a>.  It's therefore not just that one could save themselves some effort figuring out what they owe in some financial transaction, but the very existence of financial transactions themselves relies on this kind of operational closure.  Financial crises themselves, for that matter, can be seen as ultimately the same thing as bugs in computer programs: failures to compose that stem from some deficit, however small, of operational closure.  Physical technology, with all of its moving parts unavoidably coupled, also obeys this same logic, as does the validity of experimental data in science where the idea of "observing" an electron is not done through one's senses but through the syntactic propositions generated by a scientific instrument and understood only in the context of the entire body of practices, technology, and abstractions that logically compose with said propositions.</p>

	<p>So where then do the natural numbers and their operations come from?  Leopold Kronecker said that everything is a human construction except for the natural numbers, which were given to us by God.  I have no doubt he's a smarter man than myself, but he's dead wrong about this one: the natural numbers are themselves a construction that comes from the <i>reliability</i> of our natural ability to account for objects.  By this I don't mean "it turns out to be the case 100% of the time"--that's just begging the question--I mean that it's embedded in our habitual cognition the same way that computation is embedded in the activities of a personal computer and as such is a <a href="enactments_kernels">kernel</a> of our species' ancient social and technical practices no different from the way in which sensory representation is a kernel of stimulus-response mechanisms.</p>

</body>
</html>