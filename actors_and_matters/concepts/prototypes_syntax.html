<html>
<head>
	<link href="https://fonts.googleapis.com/css?family=Lora|Montserrat|Raleway&display=swap" rel="stylesheet">
	<link rel = "stylesheet"
   type = "text/css"
   href = "../../style.css" />
</head>
<body class="book">
	<h1>Prototypes and Syntax</h1>

	<i><p>"No experience would count as grounds for revising, for example, that 5 + 7 = 12. Were we to add up 5 things and 7 things and get 13 things, we would recount. Should we still, after repeated recounting, get 13 things we would assume that one of the 12 things had split or that we were seeing double or dreaming or even going mad.  The truth is that 5 + 7 = 12 is used to evaluate counting experiences, not the other way around."</p>

	<p>&mdash;Rebecca Goldstein, Incompleteness</p></i>

	<h2>There's No Such Thing as First Principles</h2>

	<p>In light of the above quote, the basic laws of addition do indeed seem to be first principles: the fact that "believing" in them is essential to being able to do science in the first place would seem to say as much.  But if something is a first principle, that would mean that it is not in any way up for any kind of "debate".  In the usual sense, this is true: nobody seriously puts 5+7=12 up for debate in the normal sense of the word.  You can philosophize about it all you want, but every time you engage in a monetary transaction or measure a piece of wood or make sure you packed enough sandwiches for the family or did work in the laboratory, you're demonstrating that you in fact do not doubt the truth of basic arithmetic.</p>

	<p>All that being said, there are places where these rules are not fully taken for granted: amongst theoretical mathematicians, for example, there are times where something like 5+7=12 is not assumed but instead derived from a set of even simpler rules known as Peano Arithmetic.  I will not go into detail here but the axioms of Peano Arithmetic only assume the existence of the number 0 and the ability to generate other numbers by defining a "successor" number, such that 1 is the successor of 0, and 2 is the successor of the successor of 0 and so on.  With a few rules defined on successors and the number 0, it's possible to prove all the basic laws of addition and multiplication on whole numbers.  So these truths are not necessarily <i>first</i> principles as one can come up with something prior to them.</p>

	<p>One may of course ask what the purpose of this is, not true only in a pedantic sense?  Consider negative numbers: these do not come from our everyday experience, you cannot see a negative number of cows or rocks.  On the other hand, the concept clealry has some import: without negative numbers, it would be impossible to balance financial transactions or do all sorts of similar things, and this even extends into the physical world where contraptions such as imaginary numbers are required for the equations that give us things like telecommunication.  People were able to come up with these ideas despite being "self evident" because of the ability to further generalize these basic rules by creating an even simpler set of assumptions.</p>

	<p>But there's no way that the idea of negative numbers could simply be posited ex-nihilo without first encountering the limits of the natural numbers.  We learn the natural numbers and their operations through a combination of experience and our parents and schoolteachers explaining addition and subtraction to us by telling us about what reliably happens when we put things together and take things away.  Negative numbers are simply a result of extending the logic of this story: is it possible to take away 4 rocks if there are only 3 to begin with?  Maybe not in the physical world, but what if you promise someone you'll give them 4 rocks because you know you'll have one more by the end of the day?  Only then do you look for "more prior" principles to expand the possible things you can do.</p>

	<h2>From Prototypes To Principles</h2>

	<p>The acts of counting, adding and subtracting positive numbers become the <i>prototype</i> for the concept of negative numbers.  The concept only exists because the question of "what happens when you subtract more than you begin with?" cries out for an answer, first quietly, when it's still only philosophical, but then gets louder and louder as the question takes on increasingly material and practical motivations.</p>

	<p>But couldn't a framework with negative numbers come out of nowhere anyway?  Surely there is nothing stopping one from creating these symbolic rules and these symbolic rules being consistent regardless of what led up to their creation.  Yes, this is absolutely true, but the problem is that symbolic rules alone do not mean anything.  I could create any number of arbitrary systems of propositions with these or that rules of inference, but they can only have any meaning if they actually *stand* for something, and not just because I said they stand for something but because people can actually find some kind of relevant congruence.  In other words, one must demonstrate some kind of <i>semantics</i>.</p>

	<p>And if you were to go back to prehistoric times and teach cavemen numbers, would you be able to make them understand what negative numbers are?  What would be the basis for accepting the idea of negative 2?  You would not be able to show them an example, because there's not yet a relevant situation that one could clearly point to; they do not yet have currency or any modern notion of debt (as opposed to ritualistic ideas of debt, which are very old).  The only way that negative numbers could become meaningful to them would be to first teach them the natural numbers and then let them ask for themselves "what happens if you take away more than you have?"; which would itself remain a merely metaphysical issue until they gradually learn on their own what such a concept would allow them to <i>do</i>.</p>

	<p>To put it another way: for negative numbers to exist as an actual general principle and not just arbitrary manipulation of logical propositions, they have to have something to generalize.  Generalizing is also not by any means an arbitrary act: the purpose of a general principle is closure.  What do I mean by this?  I mean that if you don't have a concept of negative numbers, subtraction is not an operation that is closed--if you subtract 4 from 3 you simply fall of the edge of the map and can't get back on it.  By introducing negative numbers, anything subtracted from anything else keeps you within the syntax of arithmetic.  The motivation for closing a system is itself pretty simple: the more you can guarantee that some representation of reality is syntactically closed, the more you're allowed to just mechanically crunch the symbols instead of having to consciously worry about whether what you're doing makes any sense.</p>

	<p>This logic does not just apply to an individual trying to save labor but to systems of all scales as well as to the production of more advanced knowledge in general.  The degree to which a person or an institution or a society or anything else can rely on purely syntactic operations is the degree to which it can behave in a way that embodies the logic of these operations and therefore the degree to which such behavior can be composed into more sophisticated <a href="enactments_kernels.html">enactments</a>.  It's therefore not just that one could save themselves some effort figuring out what they owe in some financial transaction, but the very existence of financial transactions themselves relies on this kind of operational closure.  Financial crises themselves, for that matter, can be seen as ultimately the same thing as bugs in computer programs: failures to compose that stem from some deficit, however small, of operational closure.  Physical technology, with all of its moving parts unavoidably coupled, also obeys this same logic, as does the validity of experimental data in science where the idea of "observing" an electron is not done through one's senses but through the syntactic propositions generated by a scientific instrument and understood only in the context of the entire body of practices, technology, and abstractions that logically compose with said propositions.</p>

	<p>So where then do the natural numbers and their operations come from?  Leopold Kronecker said that everything is a human construction except for the natural numbers, which were given to us by God.  I have no doubt he's a smarter man than myself, but he's dead wrong about this one: the natural numbers are themselves a construction that comes from the <i>reliability</i> of our natural ability to account for objects.  By this I don't mean "it turns out to be the case 100% of the time"--that's just begging the question--I mean that it's embedded in our habitual cognition the same way that computation is embedded in the activities of a personal computer and as such is a <a href="enactments_kernels">kernel</a> of our species' ancient social and technical practices no different from the way in which sensory representation is a kernel of stimulus-response mechanisms.</p>

	<h2>Emergence via Composition</h2>

	<p>Just as it's impossible for purely formal reasoning to signify anything beyond the arbitrary shuffling of symbols, it is also impossible to grasp anything without some kind of symbolic mediation.  This might seem patently false: after all, one does not need to know any formal math to make themself breakfast, but this is only true if one thinks that all formalism is overt.  There is however, no reason why one needs any kind of explicit symbols or conscious volition for a computation to happen: we might understand the behavior of a computer through the code we run on it, but this is just a projection of the <i>pattern</i> of action that makes a computer decide to light up different parts of the screen according to what runs through it.  Similarly, we can give a cashier a dollar and get back chang for our purchase because the <i>form</i> of our financial and economic system allows these actions to work with this kind of reliability and simplicity.</p>

	<p>Even deep within our own bodies this same logic takes place: we're able to become better typists, guitarists, or weightlifters by creating regularities in our neural behavior that don't then simply become one less thing to think about but themselves constitute a <i>grammar</i> that forms the building blocks of new <a href="affordances_coevolution.html">affordances</a>.  Non-pedantic formal reasoning therefore does not "point" to anything in the real world but instead <i>organizes</i> the world around it and composes kernels out of enactments.</p> 

	<p>To make myself more clear, consider this formal definition of an affordance: it's simply a potential transformation of a proposition.  By formally identifiable, I mean anything that naturally composes with some larger formal pattern: for example, one can formally tell you what a bike tire does because one can define it in relation to the larger system that is a working bicycle.  Nor is it limited to what can be explicitly identified: even if we never have words or mathematical models or diagrams for it, we can still tacitly act in ways that demonstrates formal composition between things.</p>

	<p>With affordances defined in this manner, one big question remains: what of things that are syntactically imaginable but not feasible?  To use our favorite example, a system of addition and subtraction where negative numbers are not defined leaves questions that have yet to be answered.  In a situation like this, one must invent: on paper, one may invent the negative numbers, and in one's own infant system of material exchange, one may invent the I.O.U.  A syntactic possibility with no appropriate closure is therefore itself a different kind of affordance: whereas the type previously defined is purely formal, this type of affordance is material: one <i>creates</i> something that in turn composes new forms.  This interplay of materiality and formality is the essence of how kernels "emerge" from enactments through continual <a href="affordances_coevolution">co-evolution</a>.</p>

	<h2>Metaphor</h2>

	[TODO: flesh this out by relating affordances to basic algebraic operations]

	<p>Formalism, being a way of <i>framing</i> phenomena, and constituting the way in which a material process <i>organizes</i> itself, is for all intents and purposes how such a process <i>sees</i> the world.  Once again, this doesn't just apply to constructing a formal model of something on a computer or a piece of paper: a market, for exmaple, formalizes goods in a way such that any two dollar bills are interchangeable even if the bills have tons of material differences.  But what about an organism?  Human cognition certainly isn't so cut and dry: much of the <i>function</i> of our activity might be formally definable when talking about things like replacing a bike tire or memorizing a phone number, but changing a bike tire still requires some degree of monkeying around not reducible to just the formally defined steps, and our memory is always rooted in context, leading to a need to anchor arbitrary things to something material.</p>

	<p>Nonetheless, the process is the same as above: our <i>formal</i> ideas about the world, our syntax, whether tacit or explicit, are grounded in the prototypes that we act in relation to.  Whatever parts of replacing a bike tire or memorizing a phone number are irreducibly contingent, it is still defined with regards to some formal nature.  The irreducible "monkeying around" that comes with changing a bike tire can only be defined with respect to the enactment it's part of, that enactment itself having ends that are formally defined.</p>

	<p>To put it another way: if we wish to compare two things, we have to decide on the predicates on which they're being compared--whether that predicate is number of legs, fur color, lifespan, etc.  A common prototype is how this happens--it is not about two things having the same "class" in some taxonomy, but instead about conceptualizing some idea that both can be sufficiently related to.  Once one has such an object, a prototype, one can effectively "factorize" the problem into a formal semantic space by defining things in terms of how they differ with regards to the prototype and eliding all things that never differ within the prototype (for example, when comparing horses, one would never have a criterion of "which horses can breathe underwater" or "which run on windows vs. mac OS").  In algebra, there is an operation analogous to this where one factorizes an algebraic structure by factorizing it according to some identifiable substructure, resulting in a new set of algebraic terms constituting a simpler algebraic system.</p>

	<p>Such a "semantic space" when further generalized, is just a syntax: an organization of our actions.  This organization, as noted previously, is a product of a prototype that defines our relationships; but these relationships are reflected in the nature of our interactions with said prototype--they cannot just exist in the aether a-priori.  Therefore, it's also not impossible for the prototype to not be something truly consistent, instead proving itself to be something with a formal nature that varies depending on how we approach it, similar to how you cannot observe electrons the same way that you observe planets--the former will change as a result of your observation.</p>

	<p>There is an analogue to this in algebra: depending on what "direction" you decide to approach from when operating on the substructure, you may get a different result.  However, when this is not the case and there is instead <i>symmetry</i>, you get a <i>quotient</i>, and it not only produces a simpler structure but also one that obeys all of the rules of the original larger structure; that is, its form does not in any way differ, although it inevitably shaved off some details.  The substructure the original structure gets "divided by" is the kernel, and here one may see the connection to prototypes: that if our angle of approach does not affect the nature of the prototype, then it is a kernel in this book's sense of the word, and furthermore, when one conceptualizes the world against said kernel there will be a congruence between this new syntax and the earlier one.</p>

	<p>When this is accomplished, one is able to reason within a simplified syntax about more complex syntax through the use of <i>metaphor</i>.  It's important to understand that metaphor is rooted in comparing two things with regards to their <i>formal</i> properties: specifically, by evaluating the formal properties of some prototype with regards to the syntax it's situated in.  The question of metaphorical validity and efficacy beyond mere formalism belongs to the realm of <a href="hermeneutics_decomposition">hermeneutics</a>.</p>

</body>
</html>